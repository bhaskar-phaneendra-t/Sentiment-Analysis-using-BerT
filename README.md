
# Twitter Sentiment Analysis using DistilBERT

An end-to-end NLP project that performs **binary sentiment analysis (positive / negative)** on Twitter data using **DistilBERT**.  
The project follows a **production-style ML pipeline** with modular code, logging, exception handling, early stopping, and model persistence.

---

## ğŸš€ Project Overview

- Built a complete sentiment analysis pipeline using **transformer-based NLP**
- Fine-tuned **DistilBERT** on a balanced subset of Twitter data
- Implemented **early stopping** to prevent overfitting
- Logged training progress and saved the **best-performing model**
- Designed with a clean folder structure and reusable components

---

## ğŸ§  Model & Performance

- **Model:** DistilBERT (`distilbert-base-uncased`)
- **Task:** Binary sentiment classification
- **Dataset Size:** 40,000 tweets (balanced)
- **Evaluation Metrics:** Accuracy, F1-score
- **Best Accuracy:** ~82â€“85%
- **Early Stopping:** Enabled (patience = 2)

---

## ğŸ“ Project Structure

```

sentiment_analysis_bert/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ tweets.csv
â”‚
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚   â””â”€â”€ tokenizer/
â”‚   â””â”€â”€ logs/
â”‚       â””â”€â”€ training_*.log
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ **init**.py
â”‚   â”œâ”€â”€ logger.py
â”‚   â”œâ”€â”€ exception.py
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ dataset.py
â”‚   â”œâ”€â”€ trainer.py
â”‚   â””â”€â”€ inference.py
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

````

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Create and activate virtual environment
```bash
python -m venv projectenv
projectenv\Scripts\activate
````

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ How to Train the Model

1. Place the dataset in:

```
data/tweets.csv
```

2. Run training:

```bash
python main.py
```

### What happens:

* Dataset is loaded and balanced
* DistilBERT is fine-tuned with early stopping
* Best model is saved to `artifacts/model/`
* Training logs are stored in `artifacts/logs/`

---

## ğŸ§ª Inference (Test the Trained Model)

Run:

```bash
python src/inference.py
```

Example output:

```
Positive
Negative
```

---

## ğŸ›  Technologies Used

* Python
* PyTorch
* Hugging Face Transformers
* Scikit-learn
* Pandas, NumPy
* NVIDIA CUDA (GPU acceleration)

---

## ğŸ§  Key Learnings

* Transformer-based NLP modeling
* Handling large-scale text data efficiently
* Early stopping to prevent overfitting
* Modular ML project design
* GPU-accelerated training with PyTorch

---

## ğŸ“Œ Future Improvements

* Train on larger dataset (80kâ€“150k samples)
* Upgrade to RoBERTa-base
* Add Streamlit web application
* Deploy as REST API

---

## ğŸ‘¤ Author

**Tatapudi Bhaskar Phaneendra**
Machine Learning / NLP Enthusiast

```


# Sentiment-Analysis-using-BerT
